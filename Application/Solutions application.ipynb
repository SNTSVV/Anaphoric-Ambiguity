{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import the necessary libraries and functions after installing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5)\n",
      "Collecting spacy==2.3.7\n",
      "  Downloading spacy-2.3.7-cp38-cp38-win_amd64.whl (9.7 MB)\n",
      "Requirement already satisfied: benepar==0.2.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: stanza==1.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 4)) (1.2)\n",
      "Requirement already satisfied: pydotplus==2.0.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: nimbusml==1.8.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: scikit-plot==0.3.7 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 7)) (0.3.7)\n",
      "Requirement already satisfied: imblearn==0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 8)) (0.0)\n",
      "Requirement already satisfied: tensorflow==2.4.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 9)) (2.4.1)\n",
      "Requirement already satisfied: textblob==0.15.3 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 10)) (0.15.3)\n",
      "Requirement already satisfied: sentence-transformers==2.0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from -r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (0.1.96)\n",
      "Requirement already satisfied: tokenizers>=0.9.4 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (0.10.3)\n",
      "Requirement already satisfied: torch-struct>=0.5 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (0.5)\n",
      "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from benepar==0.2.0->-r requirements.txt (line 3)) (3.17.3)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from imblearn==0.0->-r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nimbusml==1.8.0->-r requirements.txt (line 6)) (1.7.1)\n",
      "Requirement already satisfied: dotnetcore2>=2.1.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nimbusml==1.8.0->-r requirements.txt (line 6)) (2.1.21)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nimbusml==1.8.0->-r requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>0.19.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nimbusml==1.8.0->-r requirements.txt (line 6)) (0.24.2)\n",
      "Requirement already satisfied: pandas>=0.22 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nimbusml==1.8.0->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 1)) (4.62.2)\n",
      "Requirement already satisfied: click in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 1)) (2021.8.28)\n",
      "Requirement already satisfied: joblib in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk==3.5->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from pydotplus==2.0.2->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from scikit-plot==0.3.7->-r requirements.txt (line 7)) (3.4.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 11)) (0.10.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 11)) (0.0.16)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp38-cp38-win_amd64.whl (910 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from spacy==2.3.7->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.1.2)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (0.3.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (0.37.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (2.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.12.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (1.32.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorflow==2.4.1->-r requirements.txt (line 9)) (0.13.0)\n",
      "Requirement already satisfied: distro>=1.2.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from dotnetcore2>=2.1.2->nimbusml==1.8.0->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot==0.3.7->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot==0.3.7->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot==0.3.7->-r requirements.txt (line 7)) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot==0.3.7->-r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from pandas>=0.22->nimbusml==1.8.0->-r requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7->-r requirements.txt (line 2)) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7->-r requirements.txt (line 2)) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.7->-r requirements.txt (line 2)) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from scikit-learn>0.19.0->nimbusml==1.8.0->-r requirements.txt (line 6)) (2.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 9)) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from tqdm->nltk==3.5->-r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar==0.2.0->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar==0.2.0->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar==0.2.0->-r requirements.txt (line 3)) (21.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\moi\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar==0.2.0->-r requirements.txt (line 3)) (0.0.45)\n",
      "Installing collected packages: six, thinc, spacy\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "Successfully installed six-1.15.0 spacy-2.3.7 thinc-7.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T12:51:16.551750Z",
     "start_time": "2021-09-02T12:51:16.544750Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 33.0MB/s]\n",
      "2022-01-10 14:45:36 INFO: Downloading default packages for language: en (English)...\n",
      "2022-01-10 14:45:37 INFO: File exists: C:\\Users\\moi\\stanza_resources\\en\\default.zip.\n",
      "2022-01-10 14:45:42 INFO: Finished downloading models and saved to C:\\Users\\moi\\stanza_resources.\n",
      "[nltk_data] Downloading package verbnet to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package verbnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 42.6MB/s]\n",
      "2022-01-10 14:45:42 INFO: Downloading default packages for language: en (English)...\n",
      "2022-01-10 14:45:43 INFO: File exists: C:\\Users\\moi\\stanza_resources\\en\\default.zip.\n",
      "2022-01-10 14:45:47 INFO: Finished downloading models and saved to C:\\Users\\moi\\stanza_resources.\n",
      "[nltk_data] Downloading package verbnet to\n",
      "[nltk_data]     C:\\Users\\moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package verbnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import utils, importlib, sys\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the example file: **Example.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T12:51:44.946596Z",
     "start_time": "2021-09-02T12:51:44.809595Z"
    }
   },
   "outputs": [],
   "source": [
    "exampleData=pd.read_csv(\"Example.txt\",names=[\"Context\"],sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the content of the file to make sure it is properly parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T12:51:45.389602Z",
     "start_time": "2021-09-02T12:51:45.371604Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The S&amp;T component shall send all approval requ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context\n",
       "0  The S&T component shall send all approval requ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the NLP pipeline and construct the triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleData[\"Context\"]=exampleData.apply(lambda x: applynlp(x[\"Context\"],nlp),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns=[\"I\",\"me\",\"my\",\"mine\",\"myself\",\"you\",\"you\",\"your\",\"yours\",\"yourself\",\"he\",\"him\",\"his\",\"his\",\"himself\",\"she\",\"her\",\"her\",\"hers\",\"herself\",\"it\",\"it\",\"its\",\"itself\",\"we\",\"us\",\"our\",\"ours\",\"ourselves\",\"you\",\"you\",\"your\",\"yours\",\"yourselves\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\"]\n",
    "li=[]\n",
    "i,j=0,0\n",
    "ids=[]\n",
    "for context in exampleData.Context.unique():\n",
    "    for pronoun in findPronouns(context,pronouns):\n",
    "        Id=str(i)+\"-\"+pronoun.text+\"-\"+str(j)\n",
    "        while Id in ids:\n",
    "            j+=1\n",
    "            Id=str(i)+\"-\"+pronoun.text+\"-\"+str(j)\n",
    "        for candidateAntecedent in getNPs(context,pronoun):\n",
    "            li.append([Id,context,pronoun,pronoun.i,candidateAntecedent])\n",
    "            ids.append(Id)\n",
    "    i+=1\n",
    "exampleData=pd.DataFrame(li,columns=[\"Id\",\"Context\",\"Pronoun\",\"Position\",\"Candidate Antecedent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the data frame containing the triples to make sure they are properly constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Context</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Position</th>\n",
       "      <th>Candidate Antecedent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-it-0</td>\n",
       "      <td>(The, S&amp;T, component, shall, send, all, approv...</td>\n",
       "      <td>it</td>\n",
       "      <td>19</td>\n",
       "      <td>(The, S&amp;T, component)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-it-0</td>\n",
       "      <td>(The, S&amp;T, component, shall, send, all, approv...</td>\n",
       "      <td>it</td>\n",
       "      <td>19</td>\n",
       "      <td>(all, approval, requests)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-it-0</td>\n",
       "      <td>(The, S&amp;T, component, shall, send, all, approv...</td>\n",
       "      <td>it</td>\n",
       "      <td>19</td>\n",
       "      <td>(the, DBS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-it-0</td>\n",
       "      <td>(The, S&amp;T, component, shall, send, all, approv...</td>\n",
       "      <td>it</td>\n",
       "      <td>19</td>\n",
       "      <td>(the, request)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-it-0</td>\n",
       "      <td>(The, S&amp;T, component, shall, send, all, approv...</td>\n",
       "      <td>it</td>\n",
       "      <td>19</td>\n",
       "      <td>(storage, parameters)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                            Context Pronoun  \\\n",
       "0  0-it-0  (The, S&T, component, shall, send, all, approv...      it   \n",
       "1  0-it-0  (The, S&T, component, shall, send, all, approv...      it   \n",
       "2  0-it-0  (The, S&T, component, shall, send, all, approv...      it   \n",
       "3  0-it-0  (The, S&T, component, shall, send, all, approv...      it   \n",
       "4  0-it-0  (The, S&T, component, shall, send, all, approv...      it   \n",
       "\n",
       "   Position       Candidate Antecedent  \n",
       "0        19      (The, S&T, component)  \n",
       "1        19  (all, approval, requests)  \n",
       "2        19                 (the, DBS)  \n",
       "3        19             (the, request)  \n",
       "4        19      (storage, parameters)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpanBERT-based Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare SpanBERT-based Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:03:41.765141Z",
     "start_time": "2021-09-01T15:03:37.832909Z"
    }
   },
   "outputs": [],
   "source": [
    "fast_tokenizer = BertTokenizerFast.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "nlpmodel = BertForTokenClassification.from_pretrained('SpanBERT-NLPv21.8.10')\n",
    "remodel = BertForTokenClassification.from_pretrained('SpanBERT-REv21.9.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:25:56.411781Z",
     "start_time": "2021-09-01T15:25:56.389780Z"
    }
   },
   "outputs": [],
   "source": [
    "test=[]\n",
    "for Id in exampleData.Id.unique():\n",
    "    c=exampleData[exampleData.Id==Id].Context.unique()[0]\n",
    "    pronoun=exampleData[exampleData.Id==Id].Pronoun.unique()[0]\n",
    "    hashedpronoun=pronoun.text+\"#1\"\n",
    "    hashedcontext=c[:pronoun.i].text+\" \"+hashedpronoun+\" \"+c[pronoun.i+1:].text\n",
    "    test.append([Id,hashedcontext,hashedpronoun])\n",
    "testdf=pd.DataFrame(test,columns=[\"Id\",\"context\",\"pronoun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:26:13.042462Z",
     "start_time": "2021-09-01T15:26:13.028462Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = SpanDetectionData(testdf, fast_tokenizer,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: SpanBERT<sub>NLP</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:38:42.728723Z",
     "start_time": "2021-09-01T15:38:42.714723Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in nlpmodel.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:39:38.165954Z",
     "start_time": "2021-09-01T15:39:26.642975Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_steps=200,\n",
    "    logging_steps= 200,          \n",
    "    save_total_limit = 5,\n",
    "    #evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "nlp_trainer = Trainer(\n",
    "    model=nlpmodel,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:40:23.435402Z",
     "start_time": "2021-09-01T15:40:16.033615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_predictions=nlp_trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:44:15.443295Z",
     "start_time": "2021-09-01T15:44:15.422294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttruncated_predictions,tpredicted_spans=processPred(nlp_predictions,test_data,testdf,fast_tokenizer,T=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:50:57.076020Z",
     "start_time": "2021-09-01T15:50:57.065020Z"
    }
   },
   "outputs": [],
   "source": [
    "spans=[]\n",
    "for i,j in zip(testdf.index, tpredicted_spans):\n",
    "    spans.append(findspans(testdf.context[i],j))\n",
    "testdf['Resolution']=spans\n",
    "testdf['Detection']=testdf['Resolution'].apply(lambda x: \"Unambiguous\" if len(x)!=0 else \"Ambiguous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphoric ambiguity handling results of SpanBERT<sub>NLP</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[92mThe S&T component\u001b[0m\u001b[0m shall send all approval requests to the DBS. If the request contains storage parameters, \u001b[1mit#1\u001b[0m shall create a configuration record from the parameters.\n",
      "Detected as: \u001b[1mUnambiguous\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in testdf.index:\n",
    "    context=testdf.context[i]\n",
    "    pronoun=testdf.pronoun[i]\n",
    "    resolution=testdf.Resolution[i]\n",
    "    split1=context.split(pronoun)\n",
    "    if resolution and resolution[0] in split1[0]:\n",
    "        split1[0]=split1[0].replace(resolution[0],color.UNDERLINE+color.GREEN+resolution[0]+color.END+color.END)\n",
    "    print(split1[0]+'\\033[1m'+pronoun+'\\033[0m'+split1[1])\n",
    "    print(\"Detected as: \"+'\\033[1m'+testdf.Detection[i]+'\\033[0m')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: SpanBERT<sub>RE</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:52:42.174549Z",
     "start_time": "2021-09-01T15:52:33.814391Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param in remodel.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_steps=200,\n",
    "    logging_steps= 200,          \n",
    "    save_total_limit = 5,\n",
    "    #evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "re_trainer = Trainer(\n",
    "    model=remodel,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    ")\n",
    "re_predictions=re_trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:53:06.296887Z",
     "start_time": "2021-09-01T15:53:06.286885Z"
    }
   },
   "outputs": [],
   "source": [
    "ttruncated_predictions,tpredicted_spans=processPred(re_predictions,test_data,testdf,fast_tokenizer,T=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:53:07.390004Z",
     "start_time": "2021-09-01T15:53:07.374992Z"
    }
   },
   "outputs": [],
   "source": [
    "spans=[]\n",
    "for i,j in zip(testdf.index, tpredicted_spans):\n",
    "    spans.append(findspans(testdf.context[i],j))\n",
    "testdf['Resolution']=spans\n",
    "testdf['Detection']=testdf['Resolution'].apply(lambda x: \"Unambiguous\" if len(x)!=0 else \"Ambiguous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphoric ambiguity handling results of SpanBERT<sub>RE</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[92mThe S&T component\u001b[0m\u001b[0m shall send all approval requests to the DBS. If the request contains storage parameters, \u001b[1mit#1\u001b[0m shall create a configuration record from the parameters.\n",
      "Detected as: \u001b[1mUnambiguous\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in testdf.index:\n",
    "    context=testdf.context[i]\n",
    "    pronoun=testdf.pronoun[i]\n",
    "    resolution=testdf.Resolution[i]\n",
    "    split1=context.split(pronoun)\n",
    "    if resolution and resolution[0] in split1[0]:\n",
    "        split1[0]=split1[0].replace(resolution[0],color.UNDERLINE+color.GREEN+resolution[0]+color.END+color.END)\n",
    "    print(split1[0]+'\\033[1m'+pronoun+'\\033[0m'+split1[1])\n",
    "    print(\"Detected as: \"+'\\033[1m'+testdf.Detection[i]+'\\033[0m')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-based Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that the ML classifiers for anaphoric ambiguity detection are different from the ones for anaphora resoutions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 14:46:10 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-10 14:46:10 INFO: Use device: cpu\n",
      "2022-01-10 14:46:10 INFO: Loading: tokenize\n",
      "2022-01-10 14:46:10 INFO: Loading: pos\n",
      "2022-01-10 14:46:10 INFO: Loading: lemma\n",
      "2022-01-10 14:46:10 INFO: Loading: depparse\n",
      "2022-01-10 14:46:11 INFO: Loading: sentiment\n",
      "2022-01-10 14:46:12 INFO: Loading: ner\n",
      "2022-01-10 14:46:13 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "nlp1 = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T12:51:56.931757Z",
     "start_time": "2021-09-02T12:51:50.806730Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exampleDataOriginal=exampleData.copy()\n",
    "exampleData=extract_LF(exampleData,nlp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Embeddings Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T12:40:55.803373Z",
     "start_time": "2021-09-01T12:40:52.167904Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-cased',output_hidden_states = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum the embeddings derived from last hidden four layers of **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T12:43:55.424300Z",
     "start_time": "2021-09-01T12:43:50.366311Z"
    }
   },
   "outputs": [],
   "source": [
    "Hs4v = exampleData.apply(\n",
    "    lambda x: get_4layers_emb(hashdouble(x['Context'],x['Pronoun'],x['Candidate Antecedent']).strip() + \" [SEP] \" + x['Pronoun'].text +\n",
    "                           \"#1 [SEP] \" + x['Candidate Antecedent'].text+\"#2\",tokenizer,model,concat=False),\n",
    "    axis=1)\n",
    "Hs4=Hs4v.apply(lambda s: pd.Series(\n",
    "    {i: float(s[i])\n",
    "     for i in range(0, len(Hs4v[Hs4v.index[0]]))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the embeddings from **SBERT** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T12:52:10.601721Z",
     "start_time": "2021-09-01T12:51:20.616615Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\config.json\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"old_models/paraphrase-mpnet-base-v2/0_Transformer\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MPNetModel.\n",
      "\n",
      "All the weights of MPNetModel were initialized from the model checkpoint at C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetModel for predictions without further training.\n",
      "loading configuration file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\config.json\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"old_models/paraphrase-mpnet-base-v2/0_Transformer\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n",
      "Didn't find file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\added_tokens.json. We won't load it.\n",
      "loading file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\vocab.txt\n",
      "loading file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\tokenizer.json\n",
      "loading file None\n",
      "loading file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\special_tokens_map.json\n",
      "loading file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\tokenizer_config.json\n",
      "loading configuration file C:\\Users\\moi/.cache\\torch\\sentence_transformers\\sentence-transformers_paraphrase-mpnet-base-v2\\config.json\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"old_models/paraphrase-mpnet-base-v2/0_Transformer\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "smodel = SentenceTransformer('paraphrase-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T12:52:33.079047Z",
     "start_time": "2021-09-01T12:52:27.976427Z"
    }
   },
   "outputs": [],
   "source": [
    "sbertv=exampleData.apply(\n",
    "    lambda x: smodel.encode(hashdouble(x['Context'],x['Pronoun'],x['Candidate Antecedent']).strip() + \" [SEP] \" + x['Pronoun'].text +\n",
    "                           \"#1 [SEP] \" + x['Candidate Antecedent'].text+\"#2\"),\n",
    "    axis=1)\n",
    "sbert=sbertv.apply(lambda s: pd.Series(\n",
    "    {i: float(s[i])\n",
    "     for i in range(0, len(sbertv[sbertv.index[0]]))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the ML-based Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:15:58.703553Z",
     "start_time": "2021-09-02T09:15:58.697554Z"
    }
   },
   "outputs": [],
   "source": [
    "X=exampleData.drop([\"Context\",\"Pronoun\",\"Candidate Antecedent\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:19:02.057036Z",
     "start_time": "2021-09-02T09:19:02.048035Z"
    }
   },
   "outputs": [],
   "source": [
    "X.isNextVerbAnimate=X.isNextVerbAnimate.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:19:21.328094Z",
     "start_time": "2021-09-02T09:19:21.317095Z"
    }
   },
   "outputs": [],
   "source": [
    "object_cols = []\n",
    "to_remove=['Id']\n",
    "for col, types in zip(\n",
    "        X.dtypes.index,\n",
    "        X.dtypes):\n",
    "    if types == object:\n",
    "        if len(X[col].unique())<30:\n",
    "            object_cols.append(col)\n",
    "        else:\n",
    "            to_remove.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:23:23.222986Z",
     "start_time": "2021-09-02T09:23:23.218988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:22:10.510607Z",
     "start_time": "2021-09-02T09:22:10.500608Z"
    }
   },
   "outputs": [],
   "source": [
    "X=X.drop(to_remove,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:23:29.861459Z",
     "start_time": "2021-09-02T09:23:29.843460Z"
    }
   },
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X,columns=object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:21:50.664438Z",
     "start_time": "2021-09-02T09:21:50.652440Z"
    }
   },
   "outputs": [],
   "source": [
    "trainCols=loadObj(\"trainingCols.list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:23:45.155955Z",
     "start_time": "2021-09-02T09:23:45.139957Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in zip(X.isna().sum().index,X.isna().sum()):\n",
    "    if j>0:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:23:54.691659Z",
     "start_time": "2021-09-02T09:23:54.675659Z"
    }
   },
   "outputs": [],
   "source": [
    "X=X.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:26:22.510695Z",
     "start_time": "2021-09-02T09:26:22.488695Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in list(X.columns):\n",
    "    if col not in trainCols:\n",
    "        X.drop(col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:26:55.116470Z",
     "start_time": "2021-09-02T09:26:55.097470Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in trainCols:\n",
    "    if col not in X.columns:\n",
    "        X[col]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:27:15.219424Z",
     "start_time": "2021-09-02T09:27:15.216424Z"
    }
   },
   "outputs": [],
   "source": [
    "X['Id']=exampleData['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: ML<sub>LF</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphoric ambiguity detection results of ML<sub>LF</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:54:18.606751Z",
     "start_time": "2021-09-01T15:54:18.589752Z"
    }
   },
   "outputs": [],
   "source": [
    "ML_LF_Detection=loadObj(\"gn.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn=GaussianNB().set_params(**{'var_smoothing': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(var_smoothing=1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.fit(loadObj(\"X.tr\").drop(\"Id\",axis=1),loadObj(\"y.tr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 75)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObj(gn,\"gn.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ML_LF_D_predictions=ML_LF_Detection.predict_proba(X.drop('Id',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:08:23.594184Z",
     "start_time": "2021-09-02T11:08:23.569184Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detdf=getprediction(X.drop('Id',axis=1).index,ML_LF_D_predictions,X.Id,0.5,exampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to the DBS. If the request contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "Detected as: \u001b[1mAmbiguous\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in detdf.index:\n",
    "    p=detdf.Pronoun[i]\n",
    "    doc=p.doc\n",
    "    print(doc[:p.i].text+'\\033[1m'+\" \"+p.text+'\\033[0m'+\" \"+doc[p.i+1:].text)\n",
    "    print(\"Detected as: \"+'\\033[1m'+detdf.result[i]+'\\033[0m')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphora resolution results of ML<sub>LF</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:55:17.249264Z",
     "start_time": "2021-09-01T15:55:17.246260Z"
    }
   },
   "outputs": [],
   "source": [
    "ML_LF_Resolution=loadObj(\"ML_LF-resolution.Anaphora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_LF_R_predictions=pd.Series(ML_LF_Resolution.predict_proba(X.drop(\"Id\",axis=1)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:28:37.294585Z",
     "start_time": "2021-09-02T11:28:37.272585Z"
    }
   },
   "outputs": [],
   "source": [
    "resdf=getResolution(ML_LF_R_predictions,X,exampleData,theta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to the DBS. If \u001b[4m\u001b[92mthe request\u001b[0m\u001b[0m contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Id,p in zip(resdf.Id,resdf.Predicted):\n",
    "    doc=p.doc\n",
    "    pronoun=exampleData[exampleData.Id==Id].Pronoun.unique()[0]\n",
    "    if p:\n",
    "        split1=doc[:p.start].text+\" \"+color.UNDERLINE+color.GREEN+p.text+color.END+color.END+\" \"+doc[p.end:pronoun.i].text\n",
    "    print(split1+'\\033[1m'+\" \"+pronoun.text+'\\033[0m'+\" \"+doc[pronoun.i+1:].text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: ML<sub>FE</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphoric ambiguity detection results of ML<sub>FE</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:09:28.327901Z",
     "start_time": "2021-09-02T11:09:28.305902Z"
    }
   },
   "outputs": [],
   "source": [
    "ML_FE_Detection=loadObj(\"ML_FE-detection.Anaphora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_FE_D_predictions=ML_FE_Detection.predict_proba(Hs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:10:49.676079Z",
     "start_time": "2021-09-02T11:10:49.652079Z"
    }
   },
   "outputs": [],
   "source": [
    "detdf=getprediction(Hs4.index,ML_FE_D_predictions,X.Id,0.5,exampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to the DBS. If the request contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "Detected as: \u001b[1mAmbiguous\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in detdf.index:\n",
    "    p=detdf.Pronoun[i]\n",
    "    doc=p.doc\n",
    "    print(doc[:p.i].text+'\\033[1m'+\" \"+p.text+'\\033[0m'+\" \"+doc[p.i+1:].text)\n",
    "    print(\"Detected as: \"+'\\033[1m'+detdf.result[i]+'\\033[0m')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphora resolution results of ML<sub>FE</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:45:46.298421Z",
     "start_time": "2021-09-02T11:45:46.278423Z"
    }
   },
   "outputs": [],
   "source": [
    "ML_FE_Resolution=loadObj(\"ML_FE-resolution.Anaphora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_FE_R_predictions=pd.Series(ML_FE_Resolution.predict_proba(sbert).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T11:46:01.931584Z",
     "start_time": "2021-09-02T11:46:01.900585Z"
    }
   },
   "outputs": [],
   "source": [
    "resdf=getResolution(ML_FE_R_predictions,X,exampleData,theta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to \u001b[4m\u001b[92mthe DBS\u001b[0m\u001b[0m . If the request contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Id,p in zip(resdf.Id,resdf.Predicted):\n",
    "    doc=p.doc\n",
    "    pronoun=exampleData[exampleData.Id==Id].Pronoun.unique()[0]\n",
    "    if p:\n",
    "        split1=doc[:p.start].text+\" \"+color.UNDERLINE+color.GREEN+p.text+color.END+color.END+\" \"+doc[p.end:pronoun.i].text\n",
    "    print(split1+'\\033[1m'+\" \"+pronoun.text+'\\033[0m'+\" \"+doc[pronoun.i+1:].text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5: ML<sub>ensemble</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphoric ambiguity detection results of ML<sub>ensemble</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ML_EnD_predictions=ensembleprobaN(ML_FE_D_predictions,ML_LF_D_predictions,theta=0.1)\n",
    "detdf=getprediction(X.drop('Id',axis=1).index,ML_EnD_predictions,X.Id,0.5,exampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to the DBS. If the request contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "Detected as: \u001b[1mAmbiguous\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in detdf.index:\n",
    "    p=detdf.Pronoun[i]\n",
    "    doc=p.doc\n",
    "    print(doc[:p.i].text+'\\033[1m'+\" \"+p.text+'\\033[0m'+\" \"+doc[p.i+1:].text)\n",
    "    print(\"Detected as: \"+'\\033[1m'+detdf.result[i]+'\\033[0m')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The anaphora resolution results of ML<sub>ensemble</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_EnR_predictions=pd.Series(ensembleprobas(ML_FE_R_predictions,ML_LF_R_predictions).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf=getResolution(ML_EnR_predictions,X,exampleData,theta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&T component shall send all approval requests to the DBS. If \u001b[4m\u001b[92mthe request\u001b[0m\u001b[0m contains storage parameters,\u001b[1m it\u001b[0m shall create a configuration record from the parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Id,p in zip(resdf.Id,resdf.Predicted):\n",
    "    doc=p.doc\n",
    "    pronoun=exampleData[exampleData.Id==Id].Pronoun.unique()[0]\n",
    "    if p:\n",
    "        split1=doc[:p.start].text+\" \"+color.UNDERLINE+color.GREEN+p.text+color.END+color.END+\" \"+doc[p.end:pronoun.i].text\n",
    "    print(split1+'\\033[1m'+\" \"+pronoun.text+'\\033[0m'+\" \"+doc[pronoun.i+1:].text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
